{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "38d46a77-e8c9-4c58-8d63-e12b7a105bd0",
   "metadata": {},
   "source": [
    "# Load Data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91d26f09",
   "metadata": {},
   "source": [
    "This notebook outlines the end-to-end preprocessing workflow for preparing the OCR dataset for training. The goal is to ensure the data is clean, balanced, and optimized for model development."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "41f15a84-b3bb-42a0-abe8-d17632d11bd4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "%matplotlib inline\n",
    "\n",
    "import os\n",
    "import math\n",
    "import cv2\n",
    "from typing import Optional, Literal, Tuple, List"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "cf074e98-163b-40a6-b931-f4f09d6573b5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Printed Size : 1733904\n",
      "Handwritten Size : 6482\n"
     ]
    }
   ],
   "source": [
    "# ----- Load Full Dataset -----\n",
    "data = pd.read_parquet(r\"data\\raw_data\\train_raw.parquet\")\n",
    "\n",
    "# ----- Split By Source -----\n",
    "printed = data[data[\"source\"] == \"printed\"]\n",
    "written = data[data[\"source\"] == \"handwritten\"]\n",
    "\n",
    "# ----- Pre-sampling size -----\n",
    "print(\"Printed Size :\", len(printed))\n",
    "print(\"Handwritten Size :\", len(written))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "753674ce-417d-4dd6-8ff6-0cf39877996b",
   "metadata": {},
   "source": [
    "## Data Preprocessing\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae1fbea1-efb8-49a7-8b1d-df71e34566f8",
   "metadata": {},
   "source": [
    "Based on exploratory data analysis performed on this dataset, we will perform the fillowing preprocessing techniques:\n",
    "- Downsampling (printed data)\n",
    "- Conversion to Grayscale\n",
    "- Resizing to Fixed Dimensions\n",
    "- Noise Removal (conditional)\n",
    "- Binarization (conditional)\n",
    "- Normalization\n",
    "- Padding and Alignment\n",
    "- Augmentation (handwritten data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "323f3bb6-5fc8-4f38-99c4-a1649f3c96c0",
   "metadata": {},
   "source": [
    "### Down Sampling"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1c0ad34-2649-4d48-9769-988ad6bc2edd",
   "metadata": {},
   "source": [
    "We will down-sample printed and data to 100_000 samples, and augment handwritten data to increase size to 100_000. This will fix the issue of class imbalance while prserving enough data for training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "595d9e93-f811-4c35-9622-4adf5c1a622c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Handwritten Size post-sampling: 100000\n"
     ]
    }
   ],
   "source": [
    "# ----- Downsample Printed Data -----\n",
    "printed = printed.sample(n = 100000, random_state=42)\n",
    "\n",
    "# ----- Post-sampling size -----\n",
    "print(\"Handwritten Size post-sampling:\", len(printed))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "371bc7c5-a2ab-4602-91aa-8d1e32176723",
   "metadata": {},
   "source": [
    "### Preprocessing Functions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac7f7de3",
   "metadata": {},
   "source": [
    "#### Extract Image from Byte Format\n",
    "This step ensures that images stored in raw byte format can be safely processed.  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "78b6af1f-a7bb-4cfc-800d-602023f40c9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ----- Obtain Raw Bytes -----\n",
    "def get_bytes(x) -> bytes:\n",
    "    if isinstance(x, dict) and \"bytes\" in x:\n",
    "        v = x[\"bytes\"]\n",
    "        if isinstance(v, (bytes, bytearray, memoryview)):\n",
    "            return bytes(v)\n",
    "    if isinstance(x, (bytes, bytearray, memoryview)):\n",
    "        return bytes(x)\n",
    "    raise TypeError(\"Image field must be bytes or dict containing key 'bytes' with bytes\")\n",
    "\n",
    "# ----- Decode Compressed Image Bytes -----\n",
    "def decode_image_cv2(image_bytes: bytes) -> np.ndarray:\n",
    "    buf = np.frombuffer(image_bytes, dtype=np.uint8)\n",
    "    img = cv2.imdecode(buf, cv2.IMREAD_UNCHANGED)\n",
    "    if img is None:\n",
    "        raise ValueError(\"Failed to decode image bytes\")\n",
    "    return img"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0b89fe6",
   "metadata": {},
   "source": [
    "#### Grayscale Conversion  \n",
    "This step converts input images into grayscale format to simplify processing and reduce computational cost.  \n",
    "- If the image is already 2D (grayscale), it is returned as-is.  \n",
    "- If the image has 3 channels (BGR) or 4 channels (BGRA), OpenCV is used to convert it to grayscale.  \n",
    "- A fallback conversion ensures robustness in case the input has unexpected channel arrangements.  \n",
    "\n",
    "Grayscale images are essential for tasks like blur detection, thresholding, and OCR preprocessing since color information is often unnecessary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "0049d3c7-e902-4da0-a24b-04e10adc4016",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ----- Convert to Grayscale -----\n",
    "def to_grayscale(img: np.ndarray) -> np.ndarray:\n",
    "    if img.ndim == 2:\n",
    "        return img\n",
    "    if img.ndim == 3:\n",
    "        if img.shape[2] == 3:\n",
    "            return cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n",
    "        if img.shape[2] == 4:\n",
    "            return cv2.cvtColor(img, cv2.COLOR_BGRA2GRAY)\n",
    "        \n",
    "    # ----- Fallback -----\n",
    "    return cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd67daa8",
   "metadata": {},
   "source": [
    "#### Resizing to a Fixed Height  \n",
    "\n",
    "This step ensures all images have a consistent height (`target_h`, default = 128) while preserving their aspect ratio.  \n",
    "- If the image already matches the target height, it is returned unchanged.  \n",
    "- Otherwise, the width is scaled proportionally to maintain aspect ratio.  \n",
    "- **Downscaling** uses `INTER_AREA` (better for shrinking), while **upscaling** uses `INTER_LINEAR` (smoother enlargement).  \n",
    "\n",
    "This standardization is important for batching images and feeding them into models that require uniform input dimensions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "857ef9df-343a-4518-8162-df9fe56dd617",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ----- Resize to Consistent Height -----\n",
    "def resize_to_fixed_height(gray: np.ndarray, target_h: int = 128) -> np.ndarray:\n",
    "    h, w = gray.shape[:2]\n",
    "    if h == target_h:\n",
    "        return gray\n",
    "        \n",
    "    scale = target_h / float(h)\n",
    "    new_w = max(1, int(round(w * scale)))\n",
    "    method = cv2.INTER_AREA if target_h < h else cv2.INTER_LINEAR\n",
    "    \n",
    "    return cv2.resize(gray, (new_w, target_h), interpolation=method)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3237f99",
   "metadata": {},
   "source": [
    "#### Noise Removal (Optional)  \n",
    "\n",
    "This step reduces unwanted noise in grayscale images to improve clarity for OCR and preprocessing tasks.  \n",
    "- **No Denoising** → returns the image unchanged.  \n",
    "- **Median Filter** → effective for removing salt-and-pepper noise while preserving edges.  \n",
    "- **Bilateral Filter** → smooths the image while keeping edges sharp, useful for text-heavy images.  \n",
    "\n",
    "The method can be chosen via the `method` parameter (`\"median\"` or `\"bilateral\"`). If none is specified, the original image is retained.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "75b90be4-cb5a-4a99-8655-fb7fdc4bd941",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ----- Noise Removal -----\n",
    "def denoise_optional(gray: np.ndarray, method: Optional[Literal[\"median\", \"bilateral\"]] = None) -> np.ndarray:\n",
    "    if method is None:\n",
    "        return gray\n",
    "    if method == \"median\":\n",
    "        return cv2.medianBlur(gray, ksize=3)\n",
    "    if method == \"bilateral\":\n",
    "        return cv2.bilateralFilter(gray, d=5, sigmaColor=20, sigmaSpace=10)\n",
    "        \n",
    "    raise ValueError(\"Unsupported denoise method\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4ca894f",
   "metadata": {},
   "source": [
    "#### Binarization (Optional)  \n",
    "\n",
    "This step converts grayscale images into black-and-white (binary) format, making text more distinct and improving OCR performance.  \n",
    "- **No Binarization** → image remains in grayscale.  \n",
    "- **Adaptive Mean Thresholding** → threshold value is calculated as the mean of neighboring pixels within a block.  \n",
    "- **Adaptive Gaussian Thresholding** → threshold value is computed using a Gaussian-weighted sum of neighboring pixels.  \n",
    "\n",
    "Both adaptive methods help handle uneven lighting conditions and preserve readability of handwritten/printed text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "c04e8019-ab6a-4204-b01d-fadaf5b84676",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ----- Binarization -----\n",
    "def binarize_optional(gray: np.ndarray, method: Optional[Literal[\"adaptive_mean\", \"adaptive_gaussian\"]] = None) -> np.ndarray:\n",
    "    if method is None:\n",
    "        return gray\n",
    "    block_size = 25\n",
    "    C = 10\n",
    "    if method == \"adaptive_mean\":\n",
    "        return cv2.adaptiveThreshold(gray, 255, cv2.ADAPTIVE_THRESH_MEAN_C, cv2.THRESH_BINARY, block_size, C)\n",
    "    if method == \"adaptive_gaussian\":\n",
    "        return cv2.adaptiveThreshold(gray, 255, cv2.ADAPTIVE_THRESH_GAUSSIAN_C, cv2.THRESH_BINARY, block_size, C)\n",
    "    \n",
    "    raise ValueError(\"Unsupported binarization method\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "327e21a0",
   "metadata": {},
   "source": [
    "#### Normalization  \n",
    "\n",
    "This step scales pixel intensity values from **[0, 255]** to a floating-point range of **[0.0, 1.0]**.  \n",
    "Normalization ensures consistency across images, improves numerical stability, and helps models train more efficiently.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "ce9ac029-3caa-4ce7-8fd2-44e5bf344ede",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ----- Normalization -----\n",
    "def normalize_01(img_u8: np.ndarray) -> np.ndarray:\n",
    "    return (img_u8.astype(np.float32) / 255.0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7aa0aca",
   "metadata": {},
   "source": [
    "#### Padding  \n",
    "\n",
    "This step standardizes the image width to a fixed target while preserving height.  \n",
    "- If the image is wider than the target, it is **cropped**.  \n",
    "- If it is narrower, it is **padded** with a constant value (default: white background = `1.0`).  \n",
    "- Padding can be applied **left-aligned** or **center-aligned**, ensuring consistent dimensions for model training.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "5de596b4-a89e-4da9-9a21-d2964321d9fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ----- Padding -----\n",
    "def pad_to_width(img: np.ndarray, target_w: int, pad_value: float = 1.0, align: Literal[\"left\", \"center\"] = \"left\") -> np.ndarray:\n",
    "    h, w = img.shape\n",
    "    \n",
    "    if w == target_w:\n",
    "        return img\n",
    "    if w > target_w:\n",
    "        return img[:, :target_w]\n",
    "\n",
    "    out = np.full((h, target_w), fill_value=pad_value, dtype=img.dtype)\n",
    "    \n",
    "    if align == \"left\":\n",
    "        out[:, :w] = img\n",
    "    elif align == \"center\":\n",
    "        offset = (target_w - w) // 2\n",
    "        out[:, offset:offset + w] = img\n",
    "    else:\n",
    "        raise ValueError(\"Unsupported align\")\n",
    "        \n",
    "    return out"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f780b657-e5c4-4639-b3cb-18dce7236da7",
   "metadata": {},
   "source": [
    "### Preprocessing Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "deb39433-23ef-4b1a-851e-faa5a2110d8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ----- Preprocessing Pipeline For Single Image -----\n",
    "def preprocess_image_bytes(image_bytes: bytes,\n",
    "                           target_h: int = 128,\n",
    "                           denoise: Optional[Literal[\"median\", \"bilateral\"]] = None,\n",
    "                           binarize: Optional[Literal[\"adaptive_mean\", \"adaptive_gaussian\"]] = None,\n",
    "                           pad_width: Optional[int] = None,\n",
    "                           pad_align: Literal[\"left\", \"center\"] = \"left\",\n",
    "                           pad_value: float = 1.0) -> np.ndarray:\n",
    "\n",
    "    img = decode_image_cv2(image_bytes)\n",
    "    gray = to_grayscale(img)\n",
    "    gray = resize_to_fixed_height(gray, target_h=target_h)\n",
    "    gray = denoise_optional(gray, method=denoise)\n",
    "    gray = binarize_optional(gray, method=binarize)\n",
    "    arr = normalize_01(gray)  # float32 [0,1]\n",
    "\n",
    "    if pad_width is not None:\n",
    "        arr = pad_to_width(arr, target_w=pad_width, pad_value=pad_value, align=pad_align)\n",
    "        \n",
    "    return arr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "2be905b9-1b26-4bee-9a6d-98e474e67898",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ----- Preprocessing Pipeline For Complete Dataframe -----\n",
    "def preprocess_dataframe(df: pd.DataFrame,\n",
    "                         image_col: str = \"image\",\n",
    "                         label_col: str = \"text\",\n",
    "                         source_col: str = \"source\",\n",
    "                         target_h: int = 128,\n",
    "                         denoise: Optional[str] = None,\n",
    "                         binarize: Optional[str] = None,\n",
    "                         pad_width: Optional[int] = None,\n",
    "                         pad_align: str = \"left\",\n",
    "                         pad_value: float = 1.0) -> Tuple[List[np.ndarray], List[str], List[str]]:\n",
    "\n",
    "    images, labels, sources = [], [], []\n",
    "    \n",
    "    for _, row in df.iterrows():\n",
    "        b = get_bytes(row[image_col])\n",
    "        arr = preprocess_image_bytes(\n",
    "            b,\n",
    "            target_h=target_h,\n",
    "            denoise=denoise,\n",
    "            binarize=binarize,\n",
    "            pad_width=pad_width,\n",
    "            pad_align=pad_align,\n",
    "            pad_value=pad_value,\n",
    "        )\n",
    "        \n",
    "        images.append(arr)\n",
    "        labels.append(row[label_col])\n",
    "        sources.append(row[source_col])\n",
    "        \n",
    "    return images, labels, sources"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f854fd39-0202-4952-b3fa-c79d792cd64c",
   "metadata": {},
   "source": [
    "## Preprocessing Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "193ed4da-7dff-43d1-a220-292f8ca52598",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ----- Define Defaults for Current Dataset -----\n",
    "TARGET_H = 128\n",
    "DENOISE = None       \n",
    "BINARIZE = None         \n",
    "PAD_WIDTH = None        \n",
    "PAD_ALIGN = \"left\"\n",
    "PAD_VALUE = 1.0\n",
    "\n",
    "# ----- Preprocess Printed Dataset -----\n",
    "printed_imgs, printed_labels, printed_sources = preprocess_dataframe(\n",
    "    printed,\n",
    "    image_col=\"image\",\n",
    "    label_col=\"text\",\n",
    "    source_col=\"source\",\n",
    "    target_h=TARGET_H,\n",
    "    denoise=DENOISE,\n",
    "    binarize=BINARIZE,\n",
    "    pad_width=PAD_WIDTH,\n",
    "    pad_align=PAD_ALIGN,\n",
    "    pad_value=PAD_VALUE,\n",
    ")\n",
    "\n",
    "# ----- Preprocess Handwritten Dataset -----\n",
    "hand_imgs, hand_labels, hand_sources = preprocess_dataframe(\n",
    "    written,\n",
    "    image_col=\"image\",\n",
    "    label_col=\"text\",\n",
    "    source_col=\"source\",\n",
    "    target_h=TARGET_H,\n",
    "    denoise=DENOISE,\n",
    "    binarize=BINARIZE,\n",
    "    pad_width=PAD_WIDTH,\n",
    "    pad_align=PAD_ALIGN,\n",
    "    pad_value=PAD_VALUE,\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ocr_venv",
   "language": "python",
   "name": "ocr_venv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
