{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0f27d2e6-ee82-4312-9fb4-2155785970f6",
   "metadata": {},
   "source": [
    "# Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c9427a63-5813-4199-a419-b70abed239f0",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[2]\u001b[39m\u001b[32m, line 2\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mpandas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mpd\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m2\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mevaluate\u001b[39;00m\n\u001b[32m      3\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mdatasets\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m Dataset, load_dataset, DatasetDict\n\u001b[32m      4\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtransformers\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[32m      5\u001b[39m     TrOCRProcessor, \n\u001b[32m      6\u001b[39m     VisionEncoderDecoderModel,\n\u001b[32m   (...)\u001b[39m\u001b[32m      9\u001b[39m     default_data_collator\n\u001b[32m     10\u001b[39m )\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\Desktop\\Labs\\Optical Character Recognition\\ocr_venv\\Lib\\site-packages\\evaluate\\__init__.py:29\u001b[39m\n\u001b[32m     25\u001b[39m SCRIPTS_VERSION = \u001b[33m\"\u001b[39m\u001b[33mmain\u001b[39m\u001b[33m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m version.parse(__version__).is_devrelease \u001b[38;5;28;01melse\u001b[39;00m __version__\n\u001b[32m     27\u001b[39m \u001b[38;5;28;01mdel\u001b[39;00m version\n\u001b[32m---> \u001b[39m\u001b[32m29\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[34;01mevaluation_suite\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m EvaluationSuite\n\u001b[32m     30\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[34;01mevaluator\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[32m     31\u001b[39m     AudioClassificationEvaluator,\n\u001b[32m     32\u001b[39m     AutomaticSpeechRecognitionEvaluator,\n\u001b[32m   (...)\u001b[39m\u001b[32m     42\u001b[39m     evaluator,\n\u001b[32m     43\u001b[39m )\n\u001b[32m     44\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[34;01mhub\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m push_to_hub\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\Desktop\\Labs\\Optical Character Recognition\\ocr_venv\\Lib\\site-packages\\evaluate\\evaluation_suite\\__init__.py:7\u001b[39m\n\u001b[32m      4\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mpathlib\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m Path\n\u001b[32m      5\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtyping\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m Callable, Dict, Optional, Union\n\u001b[32m----> \u001b[39m\u001b[32m7\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mdatasets\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m Dataset, DownloadConfig, DownloadMode, load_dataset\n\u001b[32m      8\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mdatasets\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mutils\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mversion\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m Version\n\u001b[32m     10\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mevaluator\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m evaluator\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\Desktop\\Labs\\Optical Character Recognition\\ocr_venv\\Lib\\site-packages\\datasets\\__init__.py:17\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;66;03m# Copyright 2020 The HuggingFace Datasets Authors and the TensorFlow Datasets Authors.\u001b[39;00m\n\u001b[32m      2\u001b[39m \u001b[38;5;66;03m#\u001b[39;00m\n\u001b[32m      3\u001b[39m \u001b[38;5;66;03m# Licensed under the Apache License, Version 2.0 (the \"License\");\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m     12\u001b[39m \u001b[38;5;66;03m# See the License for the specific language governing permissions and\u001b[39;00m\n\u001b[32m     13\u001b[39m \u001b[38;5;66;03m# limitations under the License.\u001b[39;00m\n\u001b[32m     15\u001b[39m __version__ = \u001b[33m\"\u001b[39m\u001b[33m4.3.0\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m---> \u001b[39m\u001b[32m17\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[34;01marrow_dataset\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m Column, Dataset\n\u001b[32m     18\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[34;01marrow_reader\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m ReadInstruction\n\u001b[32m     19\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[34;01mbuilder\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m ArrowBasedBuilder, BuilderConfig, DatasetBuilder, GeneratorBasedBuilder\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\Desktop\\Labs\\Optical Character Recognition\\ocr_venv\\Lib\\site-packages\\datasets\\arrow_dataset.py:78\u001b[39m\n\u001b[32m     76\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m config\n\u001b[32m     77\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[34;01marrow_reader\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m ArrowReader\n\u001b[32m---> \u001b[39m\u001b[32m78\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[34;01marrow_writer\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m ArrowWriter, OptimizedTypedSequence\n\u001b[32m     79\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[34;01mdata_files\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m sanitize_patterns\n\u001b[32m     80\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[34;01mdownload\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mstreaming_download_manager\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m xgetsize\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\Desktop\\Labs\\Optical Character Recognition\\ocr_venv\\Lib\\site-packages\\datasets\\arrow_writer.py:28\u001b[39m\n\u001b[32m     25\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mfsspec\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mcore\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m url_to_fs\n\u001b[32m     27\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m config\n\u001b[32m---> \u001b[39m\u001b[32m28\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[34;01mfeatures\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m Audio, Features, Image, Pdf, Value, Video\n\u001b[32m     29\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[34;01mfeatures\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mfeatures\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[32m     30\u001b[39m     FeatureType,\n\u001b[32m     31\u001b[39m     List,\n\u001b[32m   (...)\u001b[39m\u001b[32m     39\u001b[39m     to_pyarrow_listarray,\n\u001b[32m     40\u001b[39m )\n\u001b[32m     41\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[34;01mfilesystems\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m is_remote_filesystem\n",
      "\u001b[36mFile \u001b[39m\u001b[32m<frozen importlib._bootstrap>:1360\u001b[39m, in \u001b[36m_find_and_load\u001b[39m\u001b[34m(name, import_)\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m<frozen importlib._bootstrap>:1331\u001b[39m, in \u001b[36m_find_and_load_unlocked\u001b[39m\u001b[34m(name, import_)\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m<frozen importlib._bootstrap>:935\u001b[39m, in \u001b[36m_load_unlocked\u001b[39m\u001b[34m(spec)\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m<frozen importlib._bootstrap_external>:1022\u001b[39m, in \u001b[36mexec_module\u001b[39m\u001b[34m(self, module)\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m<frozen importlib._bootstrap_external>:1118\u001b[39m, in \u001b[36mget_code\u001b[39m\u001b[34m(self, fullname)\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m<frozen importlib._bootstrap_external>:1218\u001b[39m, in \u001b[36mget_data\u001b[39m\u001b[34m(self, path)\u001b[39m\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import evaluate\n",
    "from datasets import Dataset, load_dataset, DatasetDict\n",
    "from transformers import (\n",
    "    TrOCRProcessor, \n",
    "    VisionEncoderDecoderModel,\n",
    "    TrainingArguments,\n",
    "    Trainer,\n",
    "    default_data_collator\n",
    ")\n",
    "from PIL import Image\n",
    "import os\n",
    "import io\n",
    "import torch"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "713c2a49-599c-4aab-84db-de14abd17f9d",
   "metadata": {},
   "source": [
    "# Run Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b00a8bd5-ea86-404a-8803-e65b8e37ff8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"--- RUN CONFIGURATION: PRINTED MODEL ---\")\n",
    "\n",
    "# --- 1. Define Your Project Paths ---\n",
    "PROCESSED_DATA_DIR = \"../output/processed_data/\" \n",
    "RAW_DATA_DIR = \"../../data/\"\n",
    "IMAGES_BASE_DIR = \"../../data/images/\"\n",
    "OUTPUT_DIR = \"../output/printed_model/\"\n",
    "\n",
    "# --- 2. Define Model & Data Paths ---\n",
    "MODEL_NAME = \"microsoft/trocr-base-printed\"\n",
    "TRAIN_PARQUET = \"printed_streaming.parquet\" \n",
    "VAL_PARQUET = \"val_printed.parquet\"\n",
    "\n",
    "# Final paths\n",
    "TRAIN_PARQUET_PATH = os.path.join(PROCESSED_DATA_DIR, TRAIN_PARQUET)\n",
    "VAL_PARQUET_PATH = os.path.join(IMAGES_BASE_DIR, VAL_PARQUET) \n",
    "\n",
    "print(f\"Model: {MODEL_NAME}\")\n",
    "print(f\"Output: {OUTPUT_DIR}\")\n",
    "print(f\"Training data: {TRAIN_PARQUET_PATH}\")\n",
    "print(f\"Validation data: {VAL_PARQUET_PATH}\")\n",
    "\n",
    "# --- 3. Define Column Names ---\n",
    "IMAGE_DATA_COLUMN = \"image\"\n",
    "TEXT_LABEL_COLUMN = \"text\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f3531a5-5e49-4bfd-a129-fe797fd7cb2c",
   "metadata": {},
   "source": [
    "# Model and Data Loading"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d8a1154-9dce-4e54-92ec-3f8a6e95abf1",
   "metadata": {},
   "source": [
    "## Load Processor and Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ecb493f5-c858-4e30-955e-72d5a89ef3dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- 4. Load the Processor & Model ---\n",
    "try:\n",
    "    processor = TrOCRProcessor.from_pretrained(MODEL_NAME)\n",
    "    model = VisionEncoderDecoderModel.from_pretrained(MODEL_NAME)\n",
    "    print(f\"Processor and Model loaded from {MODEL_NAME}\")\n",
    "    \n",
    "    # Set model config for fine-tuning\n",
    "    model.config.decoder_start_token_id = processor.tokenizer.cls_token_id\n",
    "    model.config.pad_token_id = processor.tokenizer.pad_token_id\n",
    "    model.config.vocab_size = model.config.decoder.vocab_size\n",
    "    \n",
    "    # Set generation config\n",
    "    model.config.eos_token_id = processor.tokenizer.sep_token_id\n",
    "    model.config.max_length = 64\n",
    "    model.config.early_stopping = True\n",
    "    model.config.no_repeat_ngram_size = 3\n",
    "    model.config.length_penalty = 2.0\n",
    "    model.config.num_beams = 4\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"Error loading processor/model: {e}\")\n",
    "    raise"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5b9de21-120f-4862-b79d-b289b33282a0",
   "metadata": {},
   "source": [
    "## Load Metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46282ad5-d969-478c-a9fc-fbb56869ad50",
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    print(f\"Loading training data from: {TRAIN_PARQUET_PATH}\")\n",
    "    # Load the training data (we know it has 'image', 'text', 'source')\n",
    "    # We use split='train' to extract the dataset from the dict\n",
    "    train_dataset = load_dataset(\"parquet\", data_files={\"train\": TRAIN_PARQUET_PATH}, split=\"train\") \n",
    "    \n",
    "    print(f\"Loading validation data from: {VAL_PARQUET_PATH}\")\n",
    "    # Load the validation data (we know it has 'image' (struct), 'label')\n",
    "    val_dataset = load_dataset(\"parquet\", data_files={\"validation\": VAL_PARQUET_PATH}, split=\"validation\")\n",
    "    \n",
    "    print(\"Successfully loaded datasets separately.\")\n",
    "    print(f\"\\nRaw train dataset: {train_dataset}\")\n",
    "    print(f\"Raw val dataset: {val_dataset}\")\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"Error loading Parquet files: {e}\")\n",
    "    raise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0ef2d61-58f2-4fab-b52b-984e1d99e0ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Standardize Column Names ---\n",
    "print(\"\\nStandardizing column schemas...\")\n",
    "\n",
    "# Rename 'label' in validation to 'text'\n",
    "if TEXT_LABEL_COLUMN not in val_dataset.column_names and 'label' in val_dataset.column_names:\n",
    "    print(\"  - Renaming 'label' to 'text' in validation set.\")\n",
    "    val_dataset = val_dataset.rename_column('label', TEXT_LABEL_COLUMN)\n",
    "\n",
    "# Remove 'source' from training data to match validation\n",
    "if 'source' in train_dataset.column_names:\n",
    "    print(\"  - Removing 'source' column from training set for consistency.\")\n",
    "    train_dataset = train_dataset.remove_columns(['source'])\n",
    "\n",
    "print(\"\\n...Standardization complete.\")\n",
    "print(f\"Cleaned train dataset features: {train_dataset.features}\")\n",
    "print(f\"Cleaned val dataset features: {val_dataset.features}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6c7acaf-4d37-4cd4-8897-4d0f7ab45d3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Downsample Validation Set\n",
    "VAL_SUBSET_SIZE = 10000 \n",
    "\n",
    "if len(val_dataset) > VAL_SUBSET_SIZE:\n",
    "    print(f\"\\nValidation set is very large ({len(val_dataset)}).\")\n",
    "    \n",
    "    # Shuffle the dataset and select a random subset\n",
    "    val_dataset = val_dataset.shuffle(seed=42).select(range(VAL_SUBSET_SIZE))\n",
    "    \n",
    "    print(f\"Using a random subset of {len(val_dataset)} samples for faster validation.\")\n",
    "else:\n",
    "    print(f\"\\nUsing full validation set of {len(val_dataset)} samples.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4eae869e-9933-4f73-8450-0144ce2100ce",
   "metadata": {},
   "source": [
    "# Data Processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "780d7bd1-1683-4a0f-9fa9-3bdede613617",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define Processing Function\n",
    "def prepare_sample(example):\n",
    "    \"\"\"\n",
    "    This function loads the image data, processes it, and tokenizes the text.\n",
    "    It can now handle either raw bytes (from train) or PIL Image objects (from val).\n",
    "    \"\"\"\n",
    "    # This variable will contain EITHER raw bytes OR a PIL.Image object\n",
    "    image_data_or_object = example[IMAGE_DATA_COLUMN]\n",
    "    text = example[TEXT_LABEL_COLUMN]\n",
    "\n",
    "    # Check for invalid text labels (None, NaN, or empty/whitespace-only string)\n",
    "    if not isinstance(text, str) or len(text.strip()) == 0:\n",
    "        return None\n",
    "    \n",
    "    try:\n",
    "        if isinstance(image_data_or_object, bytes):\n",
    "            # If we get bytes (from train_dataset), open it\n",
    "            image = Image.open(io.BytesIO(image_data_or_object)).convert(\"RGB\")\n",
    "        else:\n",
    "            # If we get a PIL Image (from val_dataset), just use it\n",
    "            # We just need to ensure it's in RGB format\n",
    "            image = image_data_or_object.convert(\"RGB\")\n",
    "\n",
    "        model_inputs = processor(\n",
    "            images=image, \n",
    "            text=text, \n",
    "            padding=\"max_length\",\n",
    "            truncation=True,\n",
    "            max_length=64\n",
    "        )\n",
    "        \n",
    "        return model_inputs\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Warning: Error processing sample with text '{str(text)[:50]}'. Skipping. Error: {e}\")\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72713305-78c7-4c29-827a-6e224a61f286",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply the Function to the Datasets\n",
    "print(\"\\nApplying processing function to datasets...\")\n",
    "\n",
    "# Make sure progress bars are off\n",
    "import datasets\n",
    "datasets.disable_progress_bar()\n",
    "\n",
    "processed_train_ds = train_dataset.map(\n",
    "    prepare_sample, \n",
    "    remove_columns=train_dataset.column_names\n",
    ")\n",
    "processed_val_ds = val_dataset.map(\n",
    "    prepare_sample, \n",
    "    remove_columns=val_dataset.column_names\n",
    ")\n",
    "\n",
    "# Filter out any samples that failed to load\n",
    "processed_train_ds = processed_train_ds.filter(lambda x: x is not None)\n",
    "processed_val_ds = processed_val_ds.filter(lambda x: x is not None)\n",
    "\n",
    "print(\"...Processing complete.\")\n",
    "print(f\"Total processed training samples: {len(processed_train_ds)}\")\n",
    "print(f\"Total processed validation samples: {len(processed_val_ds)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00ddd7aa-34bb-4973-9050-36725822a7d7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (ocr_venv)",
   "language": "python",
   "name": "ocr_venv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
