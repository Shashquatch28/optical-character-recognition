{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "0f27d2e6-ee82-4312-9fb4-2155785970f6",
      "metadata": {
        "id": "0f27d2e6-ee82-4312-9fb4-2155785970f6"
      },
      "source": [
        "# Import Libraries"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "os.environ['PYTORCH_CUDA_ALLOC_CONF'] = 'expandable_segments:True'\n",
        "\n",
        "!pip install transformers datasets evaluate jiwer tensorboard"
      ],
      "metadata": {
        "id": "qw4buW5csRiK",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "056fee29-57c7-435e-a67d-df432102cc9c"
      },
      "id": "qw4buW5csRiK",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: transformers in /usr/local/lib/python3.12/dist-packages (4.57.1)\n",
            "Requirement already satisfied: datasets in /usr/local/lib/python3.12/dist-packages (4.0.0)\n",
            "Requirement already satisfied: evaluate in /usr/local/lib/python3.12/dist-packages (0.4.6)\n",
            "Requirement already satisfied: jiwer in /usr/local/lib/python3.12/dist-packages (4.0.0)\n",
            "Requirement already satisfied: tensorboard in /usr/local/lib/python3.12/dist-packages (2.19.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from transformers) (3.20.0)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.34.0 in /usr/local/lib/python3.12/dist-packages (from transformers) (0.36.0)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.12/dist-packages (from transformers) (2.0.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.12/dist-packages (from transformers) (25.0)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.12/dist-packages (from transformers) (6.0.3)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.12/dist-packages (from transformers) (2024.11.6)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.12/dist-packages (from transformers) (2.32.4)\n",
            "Requirement already satisfied: tokenizers<=0.23.0,>=0.22.0 in /usr/local/lib/python3.12/dist-packages (from transformers) (0.22.1)\n",
            "Requirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.12/dist-packages (from transformers) (0.6.2)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.12/dist-packages (from transformers) (4.67.1)\n",
            "Requirement already satisfied: pyarrow>=15.0.0 in /usr/local/lib/python3.12/dist-packages (from datasets) (18.1.0)\n",
            "Requirement already satisfied: dill<0.3.9,>=0.3.0 in /usr/local/lib/python3.12/dist-packages (from datasets) (0.3.8)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.12/dist-packages (from datasets) (2.2.2)\n",
            "Requirement already satisfied: xxhash in /usr/local/lib/python3.12/dist-packages (from datasets) (3.6.0)\n",
            "Requirement already satisfied: multiprocess<0.70.17 in /usr/local/lib/python3.12/dist-packages (from datasets) (0.70.16)\n",
            "Requirement already satisfied: fsspec<=2025.3.0,>=2023.1.0 in /usr/local/lib/python3.12/dist-packages (from fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (2025.3.0)\n",
            "Requirement already satisfied: click>=8.1.8 in /usr/local/lib/python3.12/dist-packages (from jiwer) (8.3.0)\n",
            "Requirement already satisfied: rapidfuzz>=3.9.7 in /usr/local/lib/python3.12/dist-packages (from jiwer) (3.14.3)\n",
            "Requirement already satisfied: absl-py>=0.4 in /usr/local/lib/python3.12/dist-packages (from tensorboard) (1.4.0)\n",
            "Requirement already satisfied: grpcio>=1.48.2 in /usr/local/lib/python3.12/dist-packages (from tensorboard) (1.76.0)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.12/dist-packages (from tensorboard) (3.9)\n",
            "Requirement already satisfied: protobuf!=4.24.0,>=3.19.6 in /usr/local/lib/python3.12/dist-packages (from tensorboard) (5.29.5)\n",
            "Requirement already satisfied: setuptools>=41.0.0 in /usr/local/lib/python3.12/dist-packages (from tensorboard) (75.2.0)\n",
            "Requirement already satisfied: six>1.9 in /usr/local/lib/python3.12/dist-packages (from tensorboard) (1.17.0)\n",
            "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /usr/local/lib/python3.12/dist-packages (from tensorboard) (0.7.2)\n",
            "Requirement already satisfied: werkzeug>=1.0.1 in /usr/local/lib/python3.12/dist-packages (from tensorboard) (3.1.3)\n",
            "Requirement already satisfied: aiohttp!=4.0.0a0,!=4.0.0a1 in /usr/local/lib/python3.12/dist-packages (from fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (3.13.1)\n",
            "Requirement already satisfied: typing-extensions~=4.12 in /usr/local/lib/python3.12/dist-packages (from grpcio>=1.48.2->tensorboard) (4.15.0)\n",
            "Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub<1.0,>=0.34.0->transformers) (1.2.0)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests->transformers) (3.4.4)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests->transformers) (3.11)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests->transformers) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests->transformers) (2025.10.5)\n",
            "Requirement already satisfied: MarkupSafe>=2.1.1 in /usr/local/lib/python3.12/dist-packages (from werkzeug>=1.0.1->tensorboard) (3.0.3)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.12/dist-packages (from pandas->datasets) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.12/dist-packages (from pandas->datasets) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.12/dist-packages (from pandas->datasets) (2025.2)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.5.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (2.6.1)\n",
            "Requirement already satisfied: aiosignal>=1.4.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (1.4.0)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (25.4.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (1.8.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (6.7.0)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (0.4.1)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (1.22.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c9427a63-5813-4199-a419-b70abed239f0",
      "metadata": {
        "id": "c9427a63-5813-4199-a419-b70abed239f0"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "from datasets import Dataset, load_dataset, DatasetDict\n",
        "from transformers import (\n",
        "    TrOCRProcessor,\n",
        "    VisionEncoderDecoderModel,\n",
        "    TrainingArguments,\n",
        "    Trainer\n",
        ")\n",
        "from PIL import Image\n",
        "import os\n",
        "import torch\n",
        "import io\n",
        "import evaluate"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "d44_BHYPtBbV",
        "outputId": "e2a45998-b74a-445c-de87-a2760ee8ffe2"
      },
      "id": "d44_BHYPtBbV",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "\n",
        "# --- GPU Check ---\n",
        "if torch.cuda.is_available():\n",
        "    print(f\"✅ GPU is available!\")\n",
        "    print(f\"Device Name: {torch.cuda.get_device_name(0)}\")\n",
        "else:\n",
        "    print(f\"WARNING: GPU not available. Check Runtime > Change runtime type.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dcwiF8XZapNZ",
        "outputId": "67acc371-8d1c-4ab7-b23a-401a75f421a2"
      },
      "id": "dcwiF8XZapNZ",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✅ GPU is available!\n",
            "Device Name: Tesla T4\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "id": "713c2a49-599c-4aab-84db-de14abd17f9d",
      "metadata": {
        "id": "713c2a49-599c-4aab-84db-de14abd17f9d"
      },
      "source": [
        "# Run Configuration"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b00a8bd5-ea86-404a-8803-e65b8e37ff8f",
      "metadata": {
        "id": "b00a8bd5-ea86-404a-8803-e65b8e37ff8f",
        "outputId": "83a07418-9f25-4e0e-b783-4728c0533a2f",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--- RUN CONFIGURATION: PRINTED MODEL ---\n",
            "Found Google Drive at: /content/drive/MyDrive/\n",
            "Model: microsoft/trocr-small-printed\n",
            "Output: /content/drive/MyDrive/ml_training/output/printed_model/\n",
            "Training data: /content/drive/MyDrive/ml_training/output/processed_data/printed_streaming.parquet\n",
            "Validation data: /content/drive/MyDrive/data/images/val_printed.parquet\n"
          ]
        }
      ],
      "source": [
        "print(\"--- RUN CONFIGURATION: PRINTED MODEL ---\")\n",
        "\n",
        "# --- Define Project Paths ---\n",
        "DRIVE_PATH = \"/content/drive/MyDrive/\"\n",
        "\n",
        "# Check if the path exists to be sure\n",
        "if not os.path.exists(DRIVE_PATH):\n",
        "    print(f\"ERROR: Google Drive path not found: {DRIVE_PATH}\")\n",
        "    print(\"Please check the 'drive.mount' cell and your folder names in Google Drive.\")\n",
        "else:\n",
        "    print(f\"Found Google Drive at: {DRIVE_PATH}\")\n",
        "\n",
        "PROCESSED_DATA_DIR = os.path.join(DRIVE_PATH, \"ml_training/output/processed_data/\")\n",
        "IMAGES_BASE_DIR = os.path.join(DRIVE_PATH, \"data/images/\")\n",
        "OUTPUT_DIR = os.path.join(DRIVE_PATH, \"ml_training/output/printed_model/\")\n",
        "\n",
        "# --- Define Model & Data Paths ---\n",
        "MODEL_NAME = \"microsoft/trocr-small-printed\"\n",
        "TRAIN_PARQUET = \"printed_streaming.parquet\"\n",
        "VAL_PARQUET = \"val_printed.parquet\"\n",
        "\n",
        "# Final paths\n",
        "TRAIN_PARQUET_PATH = os.path.join(PROCESSED_DATA_DIR, TRAIN_PARQUET)\n",
        "VAL_PARQUET_PATH = os.path.join(IMAGES_BASE_DIR, VAL_PARQUET)\n",
        "\n",
        "print(f\"Model: {MODEL_NAME}\")\n",
        "print(f\"Output: {OUTPUT_DIR}\")\n",
        "print(f\"Training data: {TRAIN_PARQUET_PATH}\")\n",
        "print(f\"Validation data: {VAL_PARQUET_PATH}\")\n",
        "\n",
        "# --- Define Column Names ---\n",
        "IMAGE_DATA_COLUMN = \"image\"\n",
        "TEXT_LABEL_COLUMN = \"text\""
      ]
    },
    {
      "cell_type": "markdown",
      "id": "0f3531a5-5e49-4bfd-a129-fe797fd7cb2c",
      "metadata": {
        "id": "0f3531a5-5e49-4bfd-a129-fe797fd7cb2c"
      },
      "source": [
        "# Model and Data Loading"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "5d8a1154-9dce-4e54-92ec-3f8a6e95abf1",
      "metadata": {
        "id": "5d8a1154-9dce-4e54-92ec-3f8a6e95abf1"
      },
      "source": [
        "## Load Processor and Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ecb493f5-c858-4e30-955e-72d5a89ef3dc",
      "metadata": {
        "id": "ecb493f5-c858-4e30-955e-72d5a89ef3dc",
        "outputId": "00e03f40-c85e-49b7-cc9d-40b3d6582738",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/huggingface_hub/utils/_auth.py:94: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n",
            "Using a slow image processor as `use_fast` is unset and a slow processor was saved with this model. `use_fast=True` will be the default behavior in v4.52, even if the model was saved with a slow processor. This will result in minor differences in outputs. You'll still be able to use a slow processor with `use_fast=False`.\n",
            "Some weights of VisionEncoderDecoderModel were not initialized from the model checkpoint at microsoft/trocr-small-printed and are newly initialized: ['encoder.pooler.dense.bias', 'encoder.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processor and Model loaded from microsoft/trocr-small-printed\n"
          ]
        }
      ],
      "source": [
        "# --- Load Processor & Model ---\n",
        "try:\n",
        "    processor = TrOCRProcessor.from_pretrained(MODEL_NAME)\n",
        "    model = VisionEncoderDecoderModel.from_pretrained(MODEL_NAME)\n",
        "    print(f\"Processor and Model loaded from {MODEL_NAME}\")\n",
        "\n",
        "    # Set model config for fine-tuning\n",
        "    model.config.decoder_start_token_id = processor.tokenizer.cls_token_id\n",
        "    model.config.pad_token_id = processor.tokenizer.pad_token_id\n",
        "    model.config.vocab_size = model.config.decoder.vocab_size\n",
        "\n",
        "except Exception as e:\n",
        "    print(f\"Error loading processor/model: {e}\")\n",
        "    raise"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "46282ad5-d969-478c-a9fc-fbb56869ad50",
      "metadata": {
        "id": "46282ad5-d969-478c-a9fc-fbb56869ad50",
        "outputId": "30229b41-bcd8-466e-ea8c-5fa3f11d7815",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loading training data from: /content/drive/MyDrive/ml_training/output/processed_data/printed_streaming.parquet\n",
            "Loading validation data from: /content/drive/MyDrive/data/images/val_printed.parquet\n",
            "Successfully loaded datasets separately.\n",
            "\n",
            "Raw train dataset: Dataset({\n",
            "    features: ['image', 'text', 'source'],\n",
            "    num_rows: 100000\n",
            "})\n",
            "Raw val dataset: Dataset({\n",
            "    features: ['image', 'label'],\n",
            "    num_rows: 267578\n",
            "})\n"
          ]
        }
      ],
      "source": [
        "try:\n",
        "    print(f\"Loading training data from: {TRAIN_PARQUET_PATH}\")\n",
        "    # Load the training data (we know it has 'image', 'text', 'source')\n",
        "    # We use split='train' to extract the dataset from the dict\n",
        "    train_dataset = load_dataset(\"parquet\", data_files={\"train\": TRAIN_PARQUET_PATH}, split=\"train\")\n",
        "\n",
        "    print(f\"Loading validation data from: {VAL_PARQUET_PATH}\")\n",
        "    # Load the validation data (we know it has 'image' (struct), 'label')\n",
        "    val_dataset = load_dataset(\"parquet\", data_files={\"validation\": VAL_PARQUET_PATH}, split=\"validation\")\n",
        "\n",
        "    print(\"Successfully loaded datasets separately.\")\n",
        "    print(f\"\\nRaw train dataset: {train_dataset}\")\n",
        "    print(f\"Raw val dataset: {val_dataset}\")\n",
        "\n",
        "except Exception as e:\n",
        "    print(f\"Error loading Parquet files: {e}\")\n",
        "    raise"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f0ef2d61-58f2-4fab-b52b-984e1d99e0ed",
      "metadata": {
        "id": "f0ef2d61-58f2-4fab-b52b-984e1d99e0ed",
        "outputId": "9bed5f2d-c189-4249-bbb9-9c51a436576d",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Standardizing column schemas...\n",
            "  - Renaming 'label' to 'text' in validation set.\n",
            "  - Removing 'source' column from training set for consistency.\n",
            "\n",
            "...Standardization complete.\n",
            "Cleaned train dataset features: {'image': Value('binary'), 'text': Value('string')}\n",
            "Cleaned val dataset features: {'image': Image(mode=None, decode=True), 'text': Value('string')}\n"
          ]
        }
      ],
      "source": [
        "# --- Standardize Column Names ---\n",
        "print(\"\\nStandardizing column schemas...\")\n",
        "\n",
        "# Rename 'label' in validation to 'text'\n",
        "if TEXT_LABEL_COLUMN not in val_dataset.column_names and 'label' in val_dataset.column_names:\n",
        "    print(\"  - Renaming 'label' to 'text' in validation set.\")\n",
        "    val_dataset = val_dataset.rename_column('label', TEXT_LABEL_COLUMN)\n",
        "\n",
        "# Remove 'source' from training data to match validation\n",
        "if 'source' in train_dataset.column_names:\n",
        "    print(\"  - Removing 'source' column from training set for consistency.\")\n",
        "    train_dataset = train_dataset.remove_columns(['source'])\n",
        "\n",
        "print(\"\\n...Standardization complete.\")\n",
        "print(f\"Cleaned train dataset features: {train_dataset.features}\")\n",
        "print(f\"Cleaned val dataset features: {val_dataset.features}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e6c7acaf-4d37-4cd4-8897-4d0f7ab45d3f",
      "metadata": {
        "id": "e6c7acaf-4d37-4cd4-8897-4d0f7ab45d3f",
        "outputId": "625b3e42-8bf4-497a-b4c3-0f01edffda36",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "--- USING 10% TRAINING SUBSET: 10000 samples ---\n",
            "\n",
            "Validation set is very large (267578).\n",
            "Using a random subset of 1000 samples for faster validation.\n"
          ]
        }
      ],
      "source": [
        "# --- Subsample Training and Validation Set ---\n",
        "train_dataset = train_dataset.select(range(10000))\n",
        "print(f\"\\n--- USING 10% TRAINING SUBSET: {len(train_dataset)} samples ---\")\n",
        "\n",
        "VAL_SUBSET_SIZE = 1000\n",
        "if len(val_dataset) > VAL_SUBSET_SIZE:\n",
        "    print(f\"\\nValidation set is very large ({len(val_dataset)}).\")\n",
        "    val_dataset = val_dataset.shuffle(seed=42).select(range(VAL_SUBSET_SIZE))\n",
        "    print(f\"Using a random subset of {len(val_dataset)} samples for faster validation.\")\n",
        "else:\n",
        "    print(f\"\\nUsing full validation set of {len(val_dataset)} samples.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "00ddd7aa-34bb-4973-9050-36725822a7d7",
      "metadata": {
        "id": "00ddd7aa-34bb-4973-9050-36725822a7d7",
        "outputId": "3343ccfe-c75b-4a65-a32f-a5efcc20c5d8",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Initialized on-the-fly data collator.\n"
          ]
        }
      ],
      "source": [
        "# Define the On-the-Fly Data Collator\n",
        "\n",
        "class OnTheFlyDataCollator:\n",
        "    def __init__(self, processor):\n",
        "        self.processor = processor\n",
        "\n",
        "    def __call__(self, batch_of_examples):\n",
        "        images_to_process = []\n",
        "        text_to_process = []\n",
        "\n",
        "        for example in batch_of_examples:\n",
        "            try:\n",
        "                # Get data\n",
        "                image_data = example[IMAGE_DATA_COLUMN]\n",
        "                text = example[TEXT_LABEL_COLUMN]\n",
        "\n",
        "                # Check for bad text\n",
        "                if not isinstance(text, str) or len(text.strip()) == 0:\n",
        "                    continue\n",
        "\n",
        "                # --- This logic handles all 3 data types we've seen ---\n",
        "                if isinstance(image_data, bytes):\n",
        "                    # For train_dataset: 'image' is raw bytes\n",
        "                    image = Image.open(io.BytesIO(image_data)).convert(\"RGB\")\n",
        "                elif isinstance(image_data, dict) and 'bytes' in image_data:\n",
        "                    # For val_dataset (struct): 'image' is {'bytes': ...}\n",
        "                    image = Image.open(io.BytesIO(image_data['bytes'])).convert(\"RGB\")\n",
        "                elif isinstance(image_data, Image.Image):\n",
        "                    # For val_dataset (auto-decoded): 'image' is a PIL object\n",
        "                    image = image_data.convert(\"RGB\")\n",
        "                else:\n",
        "                    # Unrecognized type\n",
        "                    print(f\"Warning: Skipping sample with unknown image data type: {type(image_data)}\")\n",
        "                    continue\n",
        "\n",
        "                images_to_process.append(image)\n",
        "                text_to_process.append(text)\n",
        "\n",
        "            except Exception as e:\n",
        "                print(f\"Warning: Skipping corrupt sample. Error: {e}\")\n",
        "\n",
        "        # Process the entire batch at once\n",
        "        model_inputs = self.processor(\n",
        "            images=images_to_process,\n",
        "            text=text_to_process,\n",
        "            padding=\"max_length\",\n",
        "            truncation=True,\n",
        "            max_length=64,\n",
        "            return_tensors=\"pt\" # Return PyTorch tensors\n",
        "        )\n",
        "\n",
        "        return model_inputs\n",
        "\n",
        "# Initialize our new collator\n",
        "on_the_fly_collator = OnTheFlyDataCollator(processor=processor)\n",
        "print(\"Initialized on-the-fly data collator.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b0e25c26-1be3-4e9e-83b6-2d0fdec6a6c0",
      "metadata": {
        "id": "b0e25c26-1be3-4e9e-83b6-2d0fdec6a6c0",
        "outputId": "990e7c45-be28-46ec-ae70-68c3fcb0fd39",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Loaded CER metric from 'evaluate' library.\n"
          ]
        }
      ],
      "source": [
        "# --- Define Metrics ---\n",
        "\n",
        "try:\n",
        "    cer_metric = evaluate.load(\"cer\")\n",
        "    print(\"\\nLoaded CER metric from 'evaluate' library.\")\n",
        "except Exception as e:\n",
        "    print(f\"Error loading CER metric: {e}\")\n",
        "    raise\n",
        "\n",
        "def compute_metrics(pred):\n",
        "    labels_ids = pred.label_ids\n",
        "    pred_ids = pred.predictions\n",
        "    pred_str = processor.batch_decode(pred_ids, skip_special_tokens=True)\n",
        "    labels_ids[labels_ids == -100] = processor.tokenizer.pad_token_id\n",
        "    label_str = processor.batch_decode(labels_ids, skip_special_tokens=True)\n",
        "    cer = cer_metric.compute(predictions=pred_str, references=label_str)\n",
        "    return {\"cer\": cer}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a8bf37e9-0d56-4677-8399-1f85a8840f0e",
      "metadata": {
        "id": "a8bf37e9-0d56-4677-8399-1f85a8840f0e"
      },
      "outputs": [],
      "source": [
        "training_args = TrainingArguments(\n",
        "    output_dir=os.path.join(OUTPUT_DIR, \"trocr-small-checkpoints\"),\n",
        "    num_train_epochs=5,\n",
        "    per_device_train_batch_size=1,\n",
        "    per_device_eval_batch_size=1,\n",
        "    gradient_accumulation_steps=8,\n",
        "    eval_accumulation_steps=16,\n",
        "    eval_strategy=\"steps\",\n",
        "    eval_steps=500,\n",
        "    save_strategy=\"steps\",\n",
        "    save_steps=500,\n",
        "    save_total_limit=2,\n",
        "    fp16=torch.cuda.is_available(),\n",
        "    dataloader_num_workers=2,\n",
        "    remove_unused_columns=False,\n",
        "    gradient_checkpointing=True,\n",
        "    logging_dir=os.path.join(OUTPUT_DIR, \"logs\"),\n",
        "    logging_steps=100,\n",
        "    load_best_model_at_end=True,\n",
        "    metric_for_best_model=\"cer\",\n",
        "    greater_is_better=False,\n",
        "    report_to=\"tensorboard\",\n",
        "    max_grad_norm=1.0,\n",
        ")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "7fef6f93-c36e-4773-a46f-db627876c5c5",
      "metadata": {
        "id": "7fef6f93-c36e-4773-a46f-db627876c5c5"
      },
      "outputs": [],
      "source": [
        "trainer = Trainer(\n",
        "    model=model,\n",
        "    args=training_args,\n",
        "    train_dataset=train_dataset,\n",
        "    eval_dataset=val_dataset,\n",
        "    data_collator=on_the_fly_collator,\n",
        "    compute_metrics=compute_metrics,\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "fb4792da-77f9-46ca-a64f-9b8eb62cf4f9",
      "metadata": {
        "id": "fb4792da-77f9-46ca-a64f-9b8eb62cf4f9",
        "outputId": "24aee441-7102-4927-f6c1-84bf2166fb72",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 147
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "--- Starting Training ---\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "`loss_type=None` was set in the config but it is unrecognized. Using the default loss: `ForCausalLMLoss`.\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='501' max='6250' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [ 501/6250 07:36 < 1:27:42, 1.09 it/s, Epoch 0.40/5]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Step</th>\n",
              "      <th>Training Loss</th>\n",
              "      <th>Validation Loss</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "  </tbody>\n",
              "</table><p>\n",
              "    <div>\n",
              "      \n",
              "      <progress value='304' max='1000' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [ 304/1000 00:48 < 01:50, 6.31 it/s]\n",
              "    </div>\n",
              "    "
            ]
          },
          "metadata": {}
        }
      ],
      "source": [
        " # Start Training\n",
        "\n",
        "print(\"\\n--- Starting Training ---\")\n",
        "\n",
        "try:\n",
        "    trainer.train()\n",
        "    print(\"--- Training Complete ---\")\n",
        "except Exception as e:\n",
        "    print(f\"\\n--- ERROR during training ---\")\n",
        "    print(e)\n",
        "    raise"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 5
}